{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aeb187b",
   "metadata": {},
   "source": [
    "# Code analysis D-NeRF\n",
    "- Jupyter file made only one file..!!\n",
    "- Kyujin Han Made it\n",
    "- You can fully understand code, because I wrote many comments.\n",
    "\n",
    "#### References\n",
    "https://github.com/albertpumarola/D-NeRF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec631675",
   "metadata": {},
   "source": [
    "# 1. Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec798244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Module\n",
    "import os\n",
    "import imageio\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced9310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just option\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "np.random.seed(0)\n",
    "DEBUG = False\n",
    "\n",
    "# Metrics for evaluation\n",
    "img2mse = lambda x, y : torch.mean((x - y) ** 2) # This is loss function.\n",
    "mse2psnr = lambda x : -10. * torch.log(x) / torch.log(torch.Tensor([10.]))\n",
    "to8b = lambda x : (255*np.clip(x,0,1)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parser\n",
    "def config_parser():\n",
    "\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    #parser.add_argument('--config', is_config_file=True, \n",
    "    #                    help='config file path')\n",
    "    parser.add_argument(\"--expname\", type=str, default = \"hellwarrior\",\n",
    "                        help='experiment name')\n",
    "    parser.add_argument(\"--basedir\", type=str, default='./logs/', \n",
    "                        help='where to store ckpts and logs')\n",
    "    parser.add_argument(\"--datadir\", type=str, default='./data/hellwarrior/', \n",
    "                        help='input data directory')\n",
    "\n",
    "    # training options\n",
    "    parser.add_argument(\"--nerf_type\", type=str, default=\"D_NeRF\", help='nerf network type')\n",
    "    parser.add_argument(\"--N_iter\", type=int, default=500000,\n",
    "                        help='num training iterations')\n",
    "    parser.add_argument(\"--netdepth\", type=int, default=8, \n",
    "                        help='layers in network')\n",
    "    parser.add_argument(\"--netwidth\", type=int, default=256, \n",
    "                        help='channels per layer')\n",
    "    parser.add_argument(\"--netdepth_fine\", type=int, default=8, \n",
    "                        help='layers in fine network')\n",
    "    parser.add_argument(\"--netwidth_fine\", type=int, default=256, \n",
    "                        help='channels per layer in fine network')\n",
    "    parser.add_argument(\"--N_rand\", type=int, default=32*32*4, \n",
    "                        help='batch size (number of random rays per gradient step)')\n",
    "    parser.add_argument(\"--do_half_precision\", action='store_true', default=False,\n",
    "                        help='do half precision training and inference')\n",
    "    parser.add_argument(\"--lrate\", type=float, default=5e-4, \n",
    "                        help='learning rate')\n",
    "    parser.add_argument(\"--lrate_decay\", type=int, default=250, \n",
    "                        help='exponential learning rate decay (in 1000 steps)')\n",
    "    parser.add_argument(\"--chunk\", type=int, default=1024*32, \n",
    "                        help='number of rays processed in parallel, decrease if running out of memory')\n",
    "    parser.add_argument(\"--netchunk\", type=int, default=1024*64, \n",
    "                        help='number of pts sent through network in parallel, decrease if running out of memory')\n",
    "    parser.add_argument(\"--no_batching\", action='store_true', default=True,\n",
    "                        help='only take random rays from 1 image at a time')\n",
    "    parser.add_argument(\"--no_reload\", action='store_true', default=True,\n",
    "                        help='do not reload weights from saved ckpt')\n",
    "    parser.add_argument(\"--ft_path\", type=str, default=None, \n",
    "                        help='specific weights npy file to reload for coarse network')\n",
    "\n",
    "    # rendering options\n",
    "    parser.add_argument(\"--N_samples\", type=int, default=64, \n",
    "                        help='number of coarse samples per ray')\n",
    "    parser.add_argument(\"--not_zero_canonical\", action='store_true', default=False,\n",
    "                        help='if set zero time is not the canonic space')\n",
    "    parser.add_argument(\"--N_importance\", type=int, default=0,\n",
    "                        help='number of additional fine samples per ray')\n",
    "    parser.add_argument(\"--perturb\", type=float, default=1.,\n",
    "                        help='set to 0. for no jitter, 1. for jitter')\n",
    "    parser.add_argument(\"--use_viewdirs\", action='store_true', default=True,\n",
    "                        help='use full 5D input instead of 3D')\n",
    "    parser.add_argument(\"--i_embed\", type=int, default=0, \n",
    "                        help='set 0 for default positional encoding, -1 for none')\n",
    "    parser.add_argument(\"--multires\", type=int, default=10, \n",
    "                        help='log2 of max freq for positional encoding (3D location)')\n",
    "    parser.add_argument(\"--multires_views\", type=int, default=4, \n",
    "                        help='log2 of max freq for positional encoding (2D direction)')\n",
    "    parser.add_argument(\"--raw_noise_std\", type=float, default=0., \n",
    "                        help='std dev of noise added to regularize sigma_a output, 1e0 recommended')\n",
    "    parser.add_argument(\"--use_two_models_for_fine\", action='store_true', default=False,\n",
    "                        help='use two models for fine results')\n",
    "\n",
    "    parser.add_argument(\"--render_only\", action='store_true', default=False,\n",
    "                        help='do not optimize, reload weights and render out render_poses path')\n",
    "    parser.add_argument(\"--render_test\", action='store_true', default=False,\n",
    "                        help='render the test set instead of render_poses path')\n",
    "    parser.add_argument(\"--render_factor\", type=int, default=0, \n",
    "                        help='downsampling factor to speed up rendering, set 4 or 8 for fast preview')\n",
    "\n",
    "    # training options\n",
    "    parser.add_argument(\"--precrop_iters\", type=int, default=0,\n",
    "                        help='number of steps to train on central crops')\n",
    "    parser.add_argument(\"--precrop_iters_time\", type=int, default=0,\n",
    "                        help='number of steps to train on central time')\n",
    "    parser.add_argument(\"--precrop_frac\", type=float,\n",
    "                        default=.5, help='fraction of img taken for central crops')\n",
    "    parser.add_argument(\"--add_tv_loss\", action='store_true', default=False,\n",
    "                        help='evaluate tv loss')\n",
    "    parser.add_argument(\"--tv_loss_weight\", type=float,\n",
    "                        default=1.e-4, help='weight of tv loss')\n",
    "\n",
    "    # dataset options\n",
    "    parser.add_argument(\"--dataset_type\", type=str, default='blender', \n",
    "                        help='options: llff / blender / deepvoxels')\n",
    "    parser.add_argument(\"--testskip\", type=int, default=2,\n",
    "                        help='will load 1/N images from test/val sets, useful for large datasets like deepvoxels')\n",
    "\n",
    "    ## deepvoxels flags\n",
    "    parser.add_argument(\"--shape\", type=str, default='greek', \n",
    "                        help='options : armchair / cube / greek / vase')\n",
    "\n",
    "    ## blender flags\n",
    "    parser.add_argument(\"--white_bkgd\", action='store_true', default=True,\n",
    "                        help='set to render synthetic data on a white bkgd (always use for dvoxels)')\n",
    "    parser.add_argument(\"--half_res\", action='store_true', default=True,\n",
    "                        help='load blender synthetic data at 400x400 instead of 800x800')\n",
    "\n",
    "    ## llff flags\n",
    "    parser.add_argument(\"--factor\", type=int, default=8, \n",
    "                        help='downsample factor for LLFF images')\n",
    "    parser.add_argument(\"--no_ndc\", action='store_true', default=True,\n",
    "                        help='do not use normalized device coordinates (set for non-forward facing scenes)')\n",
    "    parser.add_argument(\"--lindisp\", action='store_true', default= False,\n",
    "                        help='sampling linearly in disparity rather than depth')\n",
    "    parser.add_argument(\"--spherify\", action='store_true', default=False,\n",
    "                        help='set for spherical 360 scenes')\n",
    "    parser.add_argument(\"--llffhold\", type=int, default=8, \n",
    "                        help='will take every 1/N images as LLFF test set, paper uses 8')\n",
    "\n",
    "    # logging/saving options\n",
    "    parser.add_argument(\"--i_print\",   type=int, default=1000,\n",
    "                        help='frequency of console printout and metric loggin')\n",
    "    parser.add_argument(\"--i_img\",     type=int, default=10000,\n",
    "                        help='frequency of tensorboard image logging')\n",
    "    parser.add_argument(\"--i_weights\", type=int, default=100000,\n",
    "                        help='frequency of weight ckpt saving')\n",
    "    parser.add_argument(\"--i_testset\", type=int, default=200000,\n",
    "                        help='frequency of testset saving')\n",
    "    parser.add_argument(\"--i_video\",   type=int, default=200000,\n",
    "                        help='frequency of render_poses video saving')\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2722a353",
   "metadata": {},
   "source": [
    "# 2. Datasets\n",
    "- Using blender dataset\n",
    "- 'mutant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b27db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make load_blender_dataset\n",
    "def load_blender_data(basedir, half_res=False, testskip=1):\n",
    "    splits = ['train', 'val', 'test']\n",
    "    metas = {}\n",
    "    \n",
    "    # There are 3 json file\n",
    "    # train, valid, test\n",
    "    for s in splits:\n",
    "        with open(os.path.join(basedir, 'transforms_{}.json'.format(s)), 'r') as fp:\n",
    "            metas[s] = json.load(fp)\n",
    "\n",
    "    all_imgs = []\n",
    "    all_poses = []\n",
    "    all_times = []\n",
    "    counts = [0]\n",
    "    \n",
    "    # Each json file has these items\n",
    "    # Camera_angle_x: It will be using, when calculating focal length\n",
    "    # frame name: images name\n",
    "    # time: [0,1]\n",
    "    # transformation matrix: C2W(Camera to world matrix) \n",
    "    for s in splits:\n",
    "        meta = metas[s]\n",
    "\n",
    "        imgs = []\n",
    "        poses = []\n",
    "        times = []\n",
    "        # if s=='train' or testskip==0:\n",
    "        #     skip = 2  # if you remove/change this 2, also change the /2 in the times vector\n",
    "        # else:\n",
    "        skip = testskip\n",
    "            \n",
    "        for t, frame in enumerate(meta['frames'][::skip]):\n",
    "            fname = os.path.join(basedir, frame['file_path'] + '.png')\n",
    "            imgs.append(imageio.imread(fname))\n",
    "            poses.append(np.array(frame['transform_matrix'])) # C2W append poses list.\n",
    "            cur_time = frame['time'] if 'time' in frame else float(t) / (len(meta['frames'][::skip])-1) # time\n",
    "            times.append(cur_time)\n",
    "\n",
    "        assert times[0] == 0, \"Time must start at 0\"\n",
    "        \n",
    "        # blender dataset has (RGB, density)\n",
    "        imgs = (np.array(imgs) / 255.).astype(np.float32)  # keep all 4 channels (RGBA)\n",
    "        poses = np.array(poses).astype(np.float32) # C2W\n",
    "        times = np.array(times).astype(np.float32) # t\n",
    "        counts.append(counts[-1] + imgs.shape[0]) # Each datasets # of counts\n",
    "        all_imgs.append(imgs)\n",
    "        all_poses.append(poses)\n",
    "        all_times.append(times)\n",
    "    \n",
    "    # i=0: train index\n",
    "    # i=1: valid index\n",
    "    # i=2: test index\n",
    "    i_split = [np.arange(counts[i], counts[i+1]) for i in range(3)]\n",
    "    \n",
    "    # make one matrix(concat)\n",
    "    imgs = np.concatenate(all_imgs, 0)\n",
    "    poses = np.concatenate(all_poses, 0)\n",
    "    times = np.concatenate(all_times, 0)\n",
    "    \n",
    "    H, W = imgs[0].shape[:2]\n",
    "    camera_angle_x = float(meta['camera_angle_x']) # Calculate focal length \n",
    "    focal = .5 * W / np.tan(.5 * camera_angle_x) # fx, fy will be same\n",
    "    \n",
    "    # If you have transforms_render.json file\n",
    "    if os.path.exists(os.path.join(basedir, 'transforms_{}.json'.format('render'))):\n",
    "        with open(os.path.join(basedir, 'transforms_{}.json'.format('render')), 'r') as fp:\n",
    "            meta = json.load(fp)\n",
    "        render_poses = []\n",
    "        for frame in meta['frames']:\n",
    "            render_poses.append(np.array(frame['transform_matrix']))\n",
    "        render_poses = np.array(render_poses).astype(np.float32)\n",
    "    # Usually not have, so make rendering dataset manually\n",
    "    else:\n",
    "        # Need angle, phi(-30,0), radius(4.0)\n",
    "        render_poses = torch.stack([pose_spherical(angle, -30.0, 4.0) for angle in np.linspace(-180,180,40+1)[:-1]], 0)\n",
    "    render_times = torch.linspace(0., 1., render_poses.shape[0])\n",
    "    \n",
    "    # Memory problem\n",
    "    # Reduce image resolution\n",
    "    if half_res:\n",
    "        H = H//2\n",
    "        W = W//2\n",
    "        focal = focal/2.\n",
    "\n",
    "        imgs_half_res = np.zeros((imgs.shape[0], H, W, 4))\n",
    "        for i, img in enumerate(imgs):\n",
    "            imgs_half_res[i] = cv2.resize(img, (H, W), interpolation=cv2.INTER_AREA)\n",
    "        imgs = imgs_half_res\n",
    "        # imgs = tf.image.resize_area(imgs, [400, 400]).numpy()\n",
    "\n",
    "    return imgs, poses, times, render_poses, render_times, [H, W, focal], i_split\n",
    "\n",
    "#################################################################\n",
    "# Below code, when calculate arbitrary input, use this function.\n",
    "trans_t = lambda t : torch.Tensor([\n",
    "    [1,0,0,0],\n",
    "    [0,1,0,0],\n",
    "    [0,0,1,t],\n",
    "    [0,0,0,1]]).float()\n",
    "\n",
    "rot_phi = lambda phi : torch.Tensor([\n",
    "    [1,0,0,0],\n",
    "    [0,np.cos(phi),-np.sin(phi),0],\n",
    "    [0,np.sin(phi), np.cos(phi),0],\n",
    "    [0,0,0,1]]).float()\n",
    "\n",
    "rot_theta = lambda th : torch.Tensor([\n",
    "    [np.cos(th),0,-np.sin(th),0],\n",
    "    [0,1,0,0],\n",
    "    [np.sin(th),0, np.cos(th),0],\n",
    "    [0,0,0,1]]).float()\n",
    "\n",
    "# Calculate C2W, using extrinsic parameters\n",
    "def pose_spherical(theta, phi, radius):\n",
    "    c2w = trans_t(radius)\n",
    "    c2w = rot_phi(phi/180.*np.pi) @ c2w\n",
    "    c2w = rot_theta(theta/180.*np.pi) @ c2w\n",
    "    c2w = torch.Tensor(np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]])) @ c2w\n",
    "    return c2w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35213a0",
   "metadata": {},
   "source": [
    "# 3. Make model\n",
    "- Positional embedding\n",
    "- Canonical network\n",
    "- Deformation network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f1218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, positional emebedding\n",
    "# Each coordinates applying [sin, cos] function.\n",
    "class Embedder:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "        self.create_embedding_fn() # function below\n",
    "        \n",
    "    def create_embedding_fn(self):\n",
    "        embed_fns = []\n",
    "        d = self.kwargs['input_dims'] # input dimension\n",
    "        out_dim = 0\n",
    "        if self.kwargs['include_input']: # If you want the original (x,y,z) coordinates\n",
    "            embed_fns.append(lambda x : x)\n",
    "            out_dim += d # Usually, 3\n",
    "        \n",
    "        # max_freq_log2: L-1\n",
    "        # N_freqs: L\n",
    "        max_freq = self.kwargs['max_freq_log2']\n",
    "        N_freqs = self.kwargs['num_freqs']\n",
    "        \n",
    "        # If you make positional embedding method using paper's method\n",
    "        # 1, 2, 4, 8, 16, ...\n",
    "        if self.kwargs['log_sampling']:\n",
    "            freq_bands = 2.**torch.linspace(0., max_freq, steps=N_freqs)\n",
    "            \n",
    "        # It just linear method\n",
    "        # Spacing is not equal\n",
    "        else:\n",
    "            freq_bands = torch.linspace(2.**0., 2.**max_freq, steps=N_freqs)\n",
    "        \n",
    "        for freq in freq_bands:\n",
    "            for p_fn in self.kwargs['periodic_fns']: # self.kwargs['periodic_fns'] = [torch.sin, torch.cos]\n",
    "                embed_fns.append(lambda x, p_fn=p_fn, freq=freq : p_fn(x * freq)) # make function\n",
    "                out_dim += d # Usually 63 or 63\n",
    "                    \n",
    "        self.embed_fns = embed_fns\n",
    "        self.out_dim = out_dim # Usually 60 or 63\n",
    "    \n",
    "    # Make applying positional embedding function\n",
    "    def embed(self, inputs):\n",
    "        return torch.cat([fn(inputs) for fn in self.embed_fns], -1)\n",
    "\n",
    "# get embedding function\n",
    "def get_embedder(multires, input_dims, i=0):\n",
    "    if i == -1:\n",
    "        return nn.Identity(), input_dims\n",
    "    \n",
    "    embed_kwargs = {\n",
    "                'include_input' : True,\n",
    "                'input_dims' : input_dims,\n",
    "                'max_freq_log2' : multires-1,\n",
    "                'num_freqs' : multires,\n",
    "                'log_sampling' : True,\n",
    "                'periodic_fns' : [torch.sin, torch.cos],\n",
    "    }\n",
    "    \n",
    "    embedder_obj = Embedder(**embed_kwargs)\n",
    "    embed = lambda x, eo=embedder_obj : eo.embed(x)\n",
    "    return embed, embedder_obj.out_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86907a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, make model network\n",
    "\n",
    "# First, NeRF original.\n",
    "# It is Canonical network\n",
    "class Canonical_NeRF(nn.Module):\n",
    "    def __init__(self, D=8, W=256, input_ch=3, input_ch_views=3, input_ch_time=1, output_ch=4, skips=[4],\n",
    "                 use_viewdirs=False, memory=[], embed_fn=None, output_color_ch=3, zero_canonical=True):\n",
    "        super(Canonical_NeRF, self).__init__()\n",
    "        self.D = D\n",
    "        self.W = W\n",
    "        self.input_ch = input_ch # Almost 63 or 60\n",
    "        self.input_ch_views = input_ch_views # Almost 27 or 24\n",
    "        self.skips = skips # Where input again (x,y,z)\n",
    "        self.use_viewdirs = use_viewdirs # Boolean\n",
    "\n",
    "        # self.pts_linears = nn.ModuleList(\n",
    "        #     [nn.Linear(input_ch, W)] +\n",
    "        #     [nn.Linear(W, W) if i not in self.skips else nn.Linear(W + input_ch, W) for i in range(D-1)])\n",
    "        \n",
    "        # Make linear function\n",
    "        layers = [nn.Linear(input_ch, W)]\n",
    "        for i in range(D - 1):\n",
    "            if i in memory:\n",
    "                raise NotImplementedError\n",
    "            else:\n",
    "                layer = nn.Linear\n",
    "\n",
    "            in_channels = W\n",
    "            # If next linear input again (x,y,z) n-dimension.\n",
    "            if i in self.skips:\n",
    "                in_channels += input_ch # 256 + 63 or 60\n",
    "\n",
    "            layers += [layer(in_channels, W)]\n",
    "        \n",
    "        self.pts_linears = nn.ModuleList(layers) # until, before density output layer.\n",
    "\n",
    "        self.views_linears = nn.ModuleList([nn.Linear(input_ch_views + W, W//2)]) # input view directions and next linear is 128\n",
    "        \n",
    "        # Almost we using.\n",
    "        if use_viewdirs: \n",
    "            self.feature_linear = nn.Linear(W, W) # 256 -> 256 (density output stage)\n",
    "            self.alpha_linear = nn.Linear(W, 1) # Output: Density\n",
    "            self.rgb_linear = nn.Linear(W//2, output_color_ch) # Output: RGB\n",
    "        else:\n",
    "            self.output_linear = nn.Linear(W, output_ch)\n",
    "    \n",
    "    # Canonical space not using t variables.\n",
    "    # x shape: [N, Coordinates_positional_embedding + View_directions_positional_embedding]\n",
    "    def forward(self, x):\n",
    "        input_pts, input_views = torch.split(x, [self.input_ch, self.input_ch_views], dim=-1) # So, must split\n",
    "        h = input_pts\n",
    "        for i, l in enumerate(self.pts_linears):\n",
    "            h = self.pts_linears[i](h)\n",
    "            h = F.relu(h)\n",
    "            if i in self.skips: # input again (x,y,z)\n",
    "                h = torch.cat([input_pts, h], -1)\n",
    "\n",
    "        if self.use_viewdirs:\n",
    "            alpha = self.alpha_linear(h) # Density\n",
    "            feature = self.feature_linear(h)\n",
    "            h = torch.cat([feature, input_views], -1)\n",
    "\n",
    "            for i, l in enumerate(self.views_linears):\n",
    "                h = self.views_linears[i](h)\n",
    "                h = F.relu(h)\n",
    "\n",
    "            rgb = self.rgb_linear(h) # RGB\n",
    "            outputs = torch.cat([rgb, alpha], -1) # concat, make 4-dimension\n",
    "        else:\n",
    "            outputs = self.output_linear(h)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def load_weights_from_keras(self, weights):\n",
    "        assert self.use_viewdirs, \"Not implemented if use_viewdirs=False\"\n",
    "\n",
    "        # Load pts_linears\n",
    "        for i in range(self.D):\n",
    "            idx_pts_linears = 2 * i\n",
    "            self.pts_linears[i].weight.data = torch.from_numpy(np.transpose(weights[idx_pts_linears]))\n",
    "            self.pts_linears[i].bias.data = torch.from_numpy(np.transpose(weights[idx_pts_linears+1]))\n",
    "\n",
    "        # Load feature_linear\n",
    "        idx_feature_linear = 2 * self.D\n",
    "        self.feature_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_feature_linear]))\n",
    "        self.feature_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_feature_linear+1]))\n",
    "\n",
    "        # Load views_linears\n",
    "        idx_views_linears = 2 * self.D + 2\n",
    "        self.views_linears[0].weight.data = torch.from_numpy(np.transpose(weights[idx_views_linears]))\n",
    "        self.views_linears[0].bias.data = torch.from_numpy(np.transpose(weights[idx_views_linears+1]))\n",
    "\n",
    "        # Load rgb_linear\n",
    "        idx_rbg_linear = 2 * self.D + 4\n",
    "        self.rgb_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_rbg_linear]))\n",
    "        self.rgb_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_rbg_linear+1]))\n",
    "\n",
    "        # Load alpha_linear\n",
    "        idx_alpha_linear = 2 * self.D + 6\n",
    "        self.alpha_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_alpha_linear]))\n",
    "        self.alpha_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_alpha_linear+1]))\n",
    "        \n",
    "        \n",
    "#################################################\n",
    "# Second, make Deformation network\n",
    "# And D-NeRF class function\n",
    "class D_NeRF(nn.Module):\n",
    "    def __init__(self, D=8, W=256, input_ch=3, input_ch_views=3, input_ch_time=1, output_ch=4, skips=[4],\n",
    "                 use_viewdirs=False, memory=[], embed_fn=None, zero_canonical=True):\n",
    "        super(D_NeRF, self).__init__()\n",
    "        # Also same Canonocial network\n",
    "        self.D = D\n",
    "        self.W = W\n",
    "        self.input_ch = input_ch\n",
    "        self.input_ch_views = input_ch_views \n",
    "        self.input_ch_time = input_ch_time\n",
    "        self.skips = skips\n",
    "        self.use_viewdirs = use_viewdirs\n",
    "        self.memory = memory\n",
    "        self.embed_fn = embed_fn # positional embedding function\n",
    "        self.zero_canonical = zero_canonical # Boolean, If you want not using t=0, false. // Almost True\n",
    "        \n",
    "        # Canonical network\n",
    "        self._ca_nerf = Canonical_NeRF(D=D, W=W, input_ch=input_ch, input_ch_views=input_ch_views,\n",
    "                                 input_ch_time=input_ch_time, output_ch=output_ch, skips=skips,\n",
    "                                 use_viewdirs=use_viewdirs, memory=memory, embed_fn=embed_fn, output_color_ch=3)\n",
    "        \n",
    "        #  # Create deformation network\n",
    "        self._deformation_layers, self._deformation_out_layer = self.create_deformation_net()\n",
    "    \n",
    "    # The structure almost same the Canonical network\n",
    "    def create_deformation_net(self):\n",
    "        layers = [nn.Linear(self.input_ch + self.input_ch_time, self.W)]\n",
    "        for i in range(self.D - 1):\n",
    "            if i in self.memory:\n",
    "                raise NotImplementedError\n",
    "            else:\n",
    "                layer = nn.Linear\n",
    "\n",
    "            in_channels = self.W\n",
    "            if i in self.skips: \n",
    "                in_channels += self.input_ch # 256 + 63 or 60\n",
    "            \n",
    "            layers += [layer(in_channels, self.W)]\n",
    "        return nn.ModuleList(layers), nn.Linear(self.W, 3)\n",
    "    \n",
    "    # implementation deformation network\n",
    "    def deformation_network_imp(self, new_pts, t, net, net_final):\n",
    "        h = torch.cat([new_pts, t], dim=-1) # Dimension ==> new_pts: 60 or 63 // t: 10 or 11\n",
    "        for i, l in enumerate(net):\n",
    "            h = net[i](h)\n",
    "            h = F.relu(h)\n",
    "            if i in self.skips:\n",
    "                h = torch.cat([new_pts, h], -1)\n",
    "\n",
    "        return net_final(h)\n",
    "\n",
    "    def forward(self, x, ts):\n",
    "        # X will be concat, the X positional embedding and View directions positional embedding\n",
    "        input_pts, input_views = torch.split(x, [self.input_ch, self.input_ch_views], dim=-1)\n",
    "        \n",
    "        # N: The number of t is sampling number.\n",
    "        # 10 or 11: positional embedding\n",
    "        t = ts[0] # t's shape [N,10 or 11] \n",
    "\n",
    "        # Must same! Because, D-NeRF sampling cooridnate in one image.\n",
    "        assert len(torch.unique(t[:, :1])) == 1, \"Only accepts all points from same time\" \n",
    "        \n",
    "        cur_time = t[0, 0]\n",
    "        if cur_time == 0. and self.zero_canonical: # If t = 0\n",
    "            dx = torch.zeros_like(input_pts[:, :3]) # dx = 0\n",
    "            \n",
    "        else:\n",
    "            dx = self.deformation_network_imp(input_pts, t, self._deformation_layers, self._deformation_out_layer) # Deformation network\n",
    "            input_pts_orig = input_pts[:, :3] # (x,y,z)\n",
    "            input_pts = self.embed_fn(input_pts_orig + dx) # (x+dx, y+dx, z+dx) applying position embedding\n",
    "            \n",
    "        # out = [RGB. density] 4-dimension\n",
    "        out = self._ca_nerf(torch.cat([input_pts, input_views], dim=-1)) # And input above this. \n",
    "        return out, dx\n",
    "    \n",
    "\n",
    "###########################################\n",
    "# Third, Make all NeRF class\n",
    "class NeRF:\n",
    "    @staticmethod\n",
    "    def get_by_name(type,  *args, **kwargs):\n",
    "        print (\"NeRF type selected: %s\" % type)\n",
    "\n",
    "        if type == \"original\":\n",
    "            model = Canonical_NeRF(*args, **kwargs)\n",
    "        elif type == \"D_NeRF\": # almost we using this.\n",
    "            model = D_NeRF(*args, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Type %s not recognized.\" % type)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f2f183",
   "metadata": {},
   "source": [
    "# 4. Make rendering and some function\n",
    "- create_nerf\n",
    "- run_network\n",
    "- batchify\n",
    "- render_path\n",
    "- get_rays\n",
    "- ndc_rays\n",
    "- render\n",
    "- batchify_rays\n",
    "- render_rays\n",
    "- sample_pdf\n",
    "- raw2outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7994356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make D_NeRF model.\n",
    "def create_nerf(args):\n",
    "    \"\"\"Instantiate NeRF's MLP model.\n",
    "    \"\"\"\n",
    "    # First get embedding\n",
    "    # args.multires = L\n",
    "    # args.i_embed = Do you want positional embedding?\n",
    "    embed_fn, input_ch = get_embedder(args.multires, 3, args.i_embed)\n",
    "    embedtime_fn, input_ch_time = get_embedder(args.multires, 1, args.i_embed) # t positional embedding. L=10\n",
    "\n",
    "    input_ch_views = 0\n",
    "    embeddirs_fn = None\n",
    "    if args.use_viewdirs:\n",
    "        embeddirs_fn, input_ch_views = get_embedder(args.multires_views, 3, args.i_embed) # L = 8. view directions\n",
    "\n",
    "    # N_importance: Do you use Fine network?\n",
    "    output_ch = 5 if args.N_importance > 0 else 4\n",
    "    skips = [4]\n",
    "    # Make D-NeRF\n",
    "    model = NeRF.get_by_name(args.nerf_type, D=args.netdepth, W=args.netwidth,\n",
    "                 input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "                 input_ch_views=input_ch_views, input_ch_time=input_ch_time,\n",
    "                 use_viewdirs=args.use_viewdirs, embed_fn=embed_fn,\n",
    "                 zero_canonical=not args.not_zero_canonical).to(device)\n",
    "    grad_vars = list(model.parameters()) # save coarse network\n",
    "\n",
    "    model_fine = None\n",
    "    if args.use_two_models_for_fine: # using fine network\n",
    "        model_fine = NeRF.get_by_name(args.nerf_type, D=args.netdepth_fine, W=args.netwidth_fine,\n",
    "                          input_ch=input_ch, output_ch=output_ch, skips=skips,\n",
    "                          input_ch_views=input_ch_views, input_ch_time=input_ch_time,\n",
    "                          use_viewdirs=args.use_viewdirs, embed_fn=embed_fn,\n",
    "                          zero_canonical=not args.not_zero_canonical).to(device)\n",
    "        grad_vars += list(model_fine.parameters()) # save fine network // So, D-NeRF is end-to-end training method.\n",
    "\n",
    "    # training lambda function\n",
    "    network_query_fn = lambda inputs, viewdirs, ts, network_fn : run_network(inputs, viewdirs, ts, network_fn,\n",
    "                                                                embed_fn=embed_fn,\n",
    "                                                                embeddirs_fn=embeddirs_fn,\n",
    "                                                                embedtime_fn=embedtime_fn,\n",
    "                                                                netchunk=args.netchunk,\n",
    "                                                                embd_time_discr=args.nerf_type!=\"temporal\")\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.Adam(params=grad_vars, lr=args.lrate, betas=(0.9, 0.999))\n",
    "\n",
    "    if args.do_half_precision:\n",
    "        print(\"Run model at half precision\")\n",
    "        if model_fine is not None:\n",
    "            [model, model_fine], optimizers = amp.initialize([model, model_fine], optimizer, opt_level='O1')\n",
    "        else:\n",
    "            model, optimizers = amp.initialize(model, optimizer, opt_level='O1')\n",
    "\n",
    "    start = 0\n",
    "    basedir = args.basedir\n",
    "    expname = args.expname\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    # Load checkpoints\n",
    "    if args.ft_path is not None and args.ft_path!='None':\n",
    "        ckpts = [args.ft_path]\n",
    "    else:\n",
    "        ckpts = [os.path.join(basedir, expname, f) for f in sorted(os.listdir(os.path.join(basedir, expname))) if 'tar' in f]\n",
    "\n",
    "    print('Found ckpts', ckpts)\n",
    "    if len(ckpts) > 0 and not args.no_reload:\n",
    "        ckpt_path = ckpts[-1]\n",
    "        print('Reloading from', ckpt_path)\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "\n",
    "        start = ckpt['global_step']\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "\n",
    "        # Load model\n",
    "        model.load_state_dict(ckpt['network_fn_state_dict'])\n",
    "        if model_fine is not None:\n",
    "            model_fine.load_state_dict(ckpt['network_fine_state_dict'])\n",
    "        if args.do_half_precision:\n",
    "            amp.load_state_dict(ckpt['amp'])\n",
    "\n",
    "    ##########################\n",
    "    # Make train dictinoary\n",
    "    render_kwargs_train = {\n",
    "        'network_query_fn' : network_query_fn,\n",
    "        'perturb' : args.perturb,\n",
    "        'N_importance' : args.N_importance,\n",
    "        'network_fine': model_fine,\n",
    "        'N_samples' : args.N_samples,\n",
    "        'network_fn' : model,\n",
    "        'use_viewdirs' : args.use_viewdirs,\n",
    "        'white_bkgd' : args.white_bkgd,\n",
    "        'raw_noise_std' : args.raw_noise_std,\n",
    "        'use_two_models_for_fine' : args.use_two_models_for_fine,\n",
    "    }\n",
    "\n",
    "    # NDC only good for LLFF-style forward facing data\n",
    "    if args.dataset_type != 'llff' or args.no_ndc:\n",
    "        render_kwargs_train['ndc'] = False\n",
    "        render_kwargs_train['lindisp'] = args.lindisp\n",
    "    \n",
    "    # Make test dictinoary\n",
    "    render_kwargs_test = {k : render_kwargs_train[k] for k in render_kwargs_train}\n",
    "    render_kwargs_test['perturb'] = False # Using stratified samples\n",
    "    render_kwargs_test['raw_noise_std'] = 0. # Not using noise (You can see the option in raw2outputs function.)\n",
    "    \n",
    "    # grad_vars: gradient parameters\n",
    "    return render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer\n",
    "\n",
    "#########################################\n",
    "# Second, run_network function\n",
    "# In using, network_query_fn function.\n",
    "def run_network(inputs, viewdirs, frame_time, fn, embed_fn, embeddirs_fn, embedtime_fn, netchunk=1024*64,\n",
    "                embd_time_discr=True):\n",
    "    \"\"\"Prepares inputs and applies network 'fn'.\n",
    "    inputs: N_rays x N_points_per_ray x 3\n",
    "    viewdirs: N_rays x 3\n",
    "    frame_time: N_rays x 1\n",
    "    \"\"\"\n",
    "    assert len(torch.unique(frame_time)) == 1, \"Only accepts all points from same time\"\n",
    "    cur_time = torch.unique(frame_time)[0]\n",
    "\n",
    "    # embed position\n",
    "    inputs_flat = torch.reshape(inputs, [-1, inputs.shape[-1]]) # [N_rays, N_samping, 3] -> [-1, 3]\n",
    "    embedded = embed_fn(inputs_flat)\n",
    "\n",
    "    # embed time\n",
    "    if embd_time_discr:\n",
    "        B, N, _ = inputs.shape\n",
    "        # time shape: [N_rays,]\n",
    "        input_frame_time = frame_time[:, None].expand([B, N, 1]) \n",
    "        input_frame_time_flat = torch.reshape(input_frame_time, [-1, 1])\n",
    "        embedded_time = embedtime_fn(input_frame_time_flat)\n",
    "        embedded_times = [embedded_time, embedded_time]\n",
    "\n",
    "    else:\n",
    "        assert NotImplementedError\n",
    "\n",
    "    # embed views\n",
    "    if viewdirs is not None:\n",
    "        # directions shape: [N_rays, 3]\n",
    "        input_dirs = viewdirs[:,None].expand(inputs.shape)\n",
    "        input_dirs_flat = torch.reshape(input_dirs, [-1, input_dirs.shape[-1]])\n",
    "        embedded_dirs = embeddirs_fn(input_dirs_flat)\n",
    "        embedded = torch.cat([embedded, embedded_dirs], -1) # [N_rays*N_sampling, pts+viewdirections]\n",
    "    \n",
    "    # fn: network_fn(D_NeRF)\n",
    "    outputs_flat, position_delta_flat = batchify(fn, netchunk)(embedded, embedded_times)\n",
    "    outputs = torch.reshape(outputs_flat, list(inputs.shape[:-1]) + [outputs_flat.shape[-1]])\n",
    "    position_delta = torch.reshape(position_delta_flat, list(inputs.shape[:-1]) + [position_delta_flat.shape[-1]])\n",
    "    return outputs, position_delta\n",
    "\n",
    "##########################################\n",
    "# Third, using chunk\n",
    "# It prevents memory over problem(?)\n",
    "def batchify(fn, chunk):\n",
    "    \"\"\"Constructs a version of 'fn' that applies to smaller batches.\n",
    "    \"\"\"\n",
    "    if chunk is None:\n",
    "        return fn\n",
    "    def ret(inputs_pos, inputs_time):\n",
    "        num_batches = inputs_pos.shape[0]\n",
    "\n",
    "        out_list = []\n",
    "        dx_list = []\n",
    "        for i in range(0, num_batches, chunk):\n",
    "             # Limitation input: the number of chunk\n",
    "            # Total: N_rays * N_sampling\n",
    "            # In run_network, chunk = 1024*64\n",
    "            # coordinates sampling\n",
    "            out, dx = fn(inputs_pos[i:i+chunk], [inputs_time[0][i:i+chunk], inputs_time[1][i:i+chunk]])\n",
    "            out_list += [out] # [rgb, density]\n",
    "            dx_list += [dx] # delta_x\n",
    "        return torch.cat(out_list, 0), torch.cat(dx_list, 0)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9917ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth volume rendering.\n",
    "# args.render_only = True\n",
    "def render_path(render_poses, render_times, hwf, chunk, render_kwargs, gt_imgs=None, savedir=None,\n",
    "                render_factor=0, save_also_gt=False, i_offset=0):\n",
    "    \n",
    "    # hwf get the load_blender_data: [H, W, focal]\n",
    "    H, W, focal = hwf\n",
    "\n",
    "    # low resolution\n",
    "    if render_factor!=0:\n",
    "        # Render downsampled for speed\n",
    "        H = H//render_factor\n",
    "        W = W//render_factor\n",
    "        focal = focal/render_factor\n",
    "\n",
    "    if savedir is not None:\n",
    "        save_dir_estim = os.path.join(savedir, \"estim\")\n",
    "        save_dir_gt = os.path.join(savedir, \"gt\")\n",
    "        if not os.path.exists(save_dir_estim):\n",
    "            os.makedirs(save_dir_estim)\n",
    "        if save_also_gt and not os.path.exists(save_dir_gt):\n",
    "            os.makedirs(save_dir_gt)\n",
    "\n",
    "    rgbs = []\n",
    "    disps = []\n",
    "    \n",
    "    # render_poses: make arbitrary (theta, phi, t)\n",
    "    for i, (c2w, frame_time) in enumerate(zip(tqdm(render_poses), render_times)):\n",
    "        # rendring\n",
    "        # rgb: rgb_map // but rgb will be 0~1 scale.\n",
    "        # disp: 1/density map\n",
    "        # acc: torch.sum(weights, -1) map. // Density high, acc -> 1, Density los, acc -> 0.\n",
    "        # Below, i will introduce 'render' function\n",
    "        rgb, disp, acc, _ = render(H, W, focal, chunk=chunk, c2w=c2w[:3,:4], frame_time=frame_time, **render_kwargs)\n",
    "        rgbs.append(rgb.cpu().numpy())\n",
    "        disps.append(disp.cpu().numpy())\n",
    "\n",
    "        if savedir is not None:\n",
    "            rgb8_estim = to8b(rgbs[-1]) # to8b function is rgb(0~1) => rgb(0~255)\n",
    "            filename = os.path.join(save_dir_estim, '{:03d}.png'.format(i+i_offset))\n",
    "            imageio.imwrite(filename, rgb8_estim)\n",
    "            if save_also_gt:\n",
    "                rgb8_gt = to8b(gt_imgs[i])\n",
    "                filename = os.path.join(save_dir_gt, '{:03d}.png'.format(i+i_offset))\n",
    "                imageio.imwrite(filename, rgb8_gt)\n",
    "                \n",
    "        # (kyujinpy add)\n",
    "        # Memory over problem solution\n",
    "        del rgb, disp, acc, _\n",
    "    \n",
    "    # stacking each image(frame)\n",
    "    rgbs = np.stack(rgbs, 0)\n",
    "    disps = np.stack(disps, 0)\n",
    "\n",
    "    return rgbs, disps\n",
    "\n",
    "#############################################\n",
    "# TFifth, get rays function\n",
    "def get_rays(H, W, focal, c2w):\n",
    "    i, j = torch.meshgrid(torch.linspace(0, W-1, W), torch.linspace(0, H-1, H))  # pytorch's meshgrid has indexing='ij'\n",
    "    i = i.t()\n",
    "    j = j.t()\n",
    "    dirs = torch.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -torch.ones_like(i)], -1) # Cx = 0.5*w, Cy = 0.5*H\n",
    "    # Rotate ray directions from camera frame to the world frame\n",
    "    rays_d = torch.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)  # dot product, equals to: [c2w.dot(dir) for dir in dirs]\n",
    "    # Translate camera frame's origin to the world frame. It is the origin of all rays.\n",
    "    rays_o = c2w[:3,-1].expand(rays_d.shape)\n",
    "    return rays_o, rays_d\n",
    "\n",
    "# Using the llff dataset or real forward facing scene dataset.\n",
    "def ndc_rays(H, W, focal, near, rays_o, rays_d):\n",
    "    # Shift ray origins to near plane\n",
    "    t = -(near + rays_o[...,2]) / rays_d[...,2]\n",
    "    rays_o = rays_o + t[...,None] * rays_d\n",
    "    \n",
    "    # Projection\n",
    "    o0 = -1./(W/(2.*focal)) * rays_o[...,0] / rays_o[...,2]\n",
    "    o1 = -1./(H/(2.*focal)) * rays_o[...,1] / rays_o[...,2]\n",
    "    o2 = 1. + 2. * near / rays_o[...,2]\n",
    "\n",
    "    d0 = -1./(W/(2.*focal)) * (rays_d[...,0]/rays_d[...,2] - rays_o[...,0]/rays_o[...,2])\n",
    "    d1 = -1./(H/(2.*focal)) * (rays_d[...,1]/rays_d[...,2] - rays_o[...,1]/rays_o[...,2])\n",
    "    d2 = -2. * near / rays_o[...,2]\n",
    "    \n",
    "    rays_o = torch.stack([o0,o1,o2], -1)\n",
    "    rays_d = torch.stack([d0,d1,d2], -1)\n",
    "    \n",
    "    return rays_o, \n",
    "\n",
    "#############################################\n",
    "# Sixth, more detail volume rendering code\n",
    "def render(H, W, focal, chunk=1024*32, rays=None, c2w=None, ndc=True,\n",
    "                  near=0., far=1., frame_time=None,\n",
    "                  use_viewdirs=False, c2w_staticcam=None,\n",
    "                  **kwargs):\n",
    "    \"\"\"Render rays\n",
    "    Args:\n",
    "      H: int. Height of image in pixels.\n",
    "      W: int. Width of image in pixels.\n",
    "      focal: float. Focal length of pinhole camera.\n",
    "      chunk: int. Maximum number of rays to process simultaneously. Used to\n",
    "        control maximum memory usage. Does not affect final results.\n",
    "      rays: array of shape [2, batch_size, 3]. Ray origin and direction for\n",
    "        each example in batch.\n",
    "      c2w: array of shape [3, 4]. Camera-to-world transformation matrix.\n",
    "      ndc: bool. If True, represent ray origin, direction in NDC coordinates.\n",
    "      near: float or array of shape [batch_size]. Nearest distance for a ray.\n",
    "      far: float or array of shape [batch_size]. Farthest distance for a ray.\n",
    "      use_viewdirs: bool. If True, use viewing direction of a point in space in model.\n",
    "      c2w_staticcam: array of shape [3, 4]. If not None, use this transformation matrix for \n",
    "       camera while using other c2w argument for viewing directions.\n",
    "    Returns:\n",
    "      rgb_map: [batch_size, 3]. Predicted RGB values for rays.\n",
    "      disp_map: [batch_size]. Disparity map. Inverse of depth.\n",
    "      acc_map: [batch_size]. Accumulated opacity (alpha) along a ray.\n",
    "      extras: dict with everything returned by render_rays().\n",
    "    \"\"\"\n",
    "    if c2w is not None:\n",
    "        # special case to render full image\n",
    "        rays_o, rays_d = get_rays(H, W, focal, c2w) # almost using. c2w = transformation_matrix in json file.\n",
    "        \n",
    "    else:\n",
    "        # use provided ray batch\n",
    "        rays_o, rays_d = rays\n",
    "\n",
    "    if use_viewdirs:\n",
    "        # provide ray directions as input\n",
    "        viewdirs = rays_d\n",
    "        if c2w_staticcam is not None:\n",
    "            # special case to visualize effect of viewdirs\n",
    "            rays_o, rays_d = get_rays(H, W, focal, c2w_staticcam)\n",
    "        viewdirs = viewdirs / torch.norm(viewdirs, dim=-1, keepdim=True) # Normalize. Because made -> magnitue 1.\n",
    "        viewdirs = torch.reshape(viewdirs, [-1,3]).float() # [N_rays, 3]\n",
    "\n",
    "    sh = rays_d.shape # [..., 3]\n",
    "    # Using llff, forward facing scene dataset.\n",
    "    if ndc:\n",
    "        # for forward facing scenes\n",
    "        rays_o, rays_d = ndc_rays(H, W, focal, 1., rays_o, rays_d)\n",
    "\n",
    "    # Create ray batch\n",
    "    # rays_o: [H, W, 3]\n",
    "    # rays_d: [H, W, 3]\n",
    "    rays_o = torch.reshape(rays_o, [-1,3]).float() # [N_rays, 3]\n",
    "    rays_d = torch.reshape(rays_d, [-1,3]).float() # [N_rays, 3]\n",
    "    \n",
    "    # Set near, far \n",
    "    # (set rays minimum and maximum)\n",
    "    near, far = near * torch.ones_like(rays_d[...,:1]), far * torch.ones_like(rays_d[...,:1])\n",
    "    frame_time = frame_time * torch.ones_like(rays_d[...,:1])\n",
    "    rays = torch.cat([rays_o, rays_d, near, far, frame_time], -1)\n",
    "    \n",
    "    if use_viewdirs:\n",
    "        rays = torch.cat([rays, viewdirs], -1)\n",
    "    # rays = [N_rays, ray_o(3)+rays_d(3)+near(1)+far(!)+frame_time(!)+viewdirs(3)] ==> [N_rays, 12]\n",
    "\n",
    "    # Render and reshape\n",
    "    # chunk: prevents memory over problem. Set input sampling number.\n",
    "    all_ret = batchify_rays(rays, chunk, **kwargs)\n",
    "    for k in all_ret:\n",
    "        k_sh = list(sh[:-1]) + list(all_ret[k].shape[1:])\n",
    "        all_ret[k] = torch.reshape(all_ret[k], k_sh)\n",
    "    \n",
    "    # Output\n",
    "    k_extract = ['rgb_map', 'disp_map', 'acc_map']\n",
    "    ret_list = [all_ret[k] for k in k_extract]\n",
    "    ret_dict = {k : all_ret[k] for k in all_ret if k not in k_extract}\n",
    "    return ret_list + [ret_dict]\n",
    "\n",
    "###################################\n",
    "# Seventh, make batch(?) and training.\n",
    "# Ray sampling\n",
    "def batchify_rays(rays_flat, chunk=1024*32, **kwargs):\n",
    "    \"\"\"Render rays in smaller minibatches to avoid OOM.\n",
    "    \"\"\"\n",
    "    all_ret = {}\n",
    "    for i in range(0, rays_flat.shape[0], chunk):\n",
    "        # Total: H*W (400*400)\n",
    "        # The limitation rays input 1024*32.\n",
    "        ret = render_rays(rays_flat[i:i+chunk], **kwargs)\n",
    "        for k in ret:\n",
    "            if k not in all_ret:\n",
    "                all_ret[k] = []\n",
    "            all_ret[k].append(ret[k])\n",
    "\n",
    "    all_ret = {k : torch.cat(all_ret[k], 0) for k in all_ret}\n",
    "    return all_ret\n",
    "\n",
    "###################################################\n",
    "# Eighth, render_rays function.\n",
    "# Very important function\n",
    "# Training\n",
    "# Coordinate sampling\n",
    "def render_rays(ray_batch,\n",
    "                network_fn,\n",
    "                network_query_fn,\n",
    "                N_samples,\n",
    "                retraw=False,\n",
    "                lindisp=False,\n",
    "                perturb=0.,\n",
    "                N_importance=0,\n",
    "                network_fine=None,\n",
    "                white_bkgd=False,\n",
    "                raw_noise_std=0.,\n",
    "                verbose=False,\n",
    "                pytest=False,\n",
    "                z_vals=None,\n",
    "                use_two_models_for_fine=False):\n",
    "    \"\"\"Volumetric rendering.\n",
    "    Args:\n",
    "      ray_batch: array of shape [batch_size, ...]. All information necessary\n",
    "        for sampling along a ray, including: ray origin, ray direction, min\n",
    "        dist, max dist, and unit-magnitude viewing direction.\n",
    "      network_fn: function. Model for predicting RGB and density at each point\n",
    "        in space.\n",
    "      network_query_fn: function used for passing queries to network_fn.\n",
    "      N_samples: int. Number of different times to sample along each ray.\n",
    "      retraw: bool. If True, include model's raw, unprocessed predictions.\n",
    "      lindisp: bool. If True, sample linearly in inverse depth rather than in depth.\n",
    "      perturb: float, 0 or 1. If non-zero, each ray is sampled at stratified\n",
    "        random points in time.\n",
    "      N_importance: int. Number of additional times to sample along each ray.\n",
    "        These samples are only passed to network_fine.\n",
    "      network_fine: \"fine\" network with same spec as network_fn.\n",
    "      white_bkgd: bool. If True, assume a white background.\n",
    "      raw_noise_std: ...\n",
    "      verbose: bool. If True, print more debugging info.\n",
    "    Returns:\n",
    "      rgb_map: [num_rays, 3]. Estimated RGB color of a ray. Comes from fine model.\n",
    "      disp_map: [num_rays]. Disparity map. 1 / depth.\n",
    "      acc_map: [num_rays]. Accumulated opacity along each ray. Comes from fine model.\n",
    "      raw: [num_rays, num_samples, 4]. Raw predictions from model.\n",
    "      rgb0: See rgb_map. Output for coarse model.\n",
    "      disp0: See disp_map. Output for coarse model.\n",
    "      acc0: See acc_map. Output for coarse model.\n",
    "      z_std: [num_rays]. Standard deviation of distances along ray for each\n",
    "        sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We remember\n",
    "    # rays = [N_rays, ray_o(3)+rays_d(3)+near(1)+far(!)+frame_time(!)+viewdirs(3)] ==> [N_rays, 12]\n",
    "    # N_rays: default 1024*32\n",
    "    # Each rays, has z locations // Coarse network: 64, Fine network: 128 (Default)\n",
    "\n",
    "    N_rays = ray_batch.shape[0]\n",
    "    rays_o, rays_d = ray_batch[:,0:3], ray_batch[:,3:6] # [N_rays, 3] each\n",
    "    viewdirs = ray_batch[:,-3:] if ray_batch.shape[-1] > 9 else None\n",
    "    bounds = torch.reshape(ray_batch[...,6:9], [-1,1,3]) # near, far, time\n",
    "    near, far, frame_time = bounds[...,0], bounds[...,1], bounds[...,2]\n",
    "    z_samples = None\n",
    "    rgb_map_0, disp_map_0, acc_map_0, position_delta_0 = None, None, None, None\n",
    "    \n",
    "    # Calculate sampling z location\n",
    "    if z_vals is None:\n",
    "        t_vals = torch.linspace(0., 1., steps=N_samples) # N_samples: the number of sampling coordinates.\n",
    "        \n",
    "        # Trianing time we used it.\n",
    "        if not lindisp:\n",
    "            z_vals = near * (1.-t_vals) + far * (t_vals) # Make z location in rays direction\n",
    "        else:\n",
    "            z_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
    "\n",
    "        z_vals = z_vals.expand([N_rays, N_samples]) # [N_rays, N_samples]\n",
    "    \n",
    "        # stratified samples\n",
    "        # Nc sampling\n",
    "        if perturb > 0.:\n",
    "            # get intervals between samples\n",
    "            mids = .5 * (z_vals[...,1:] + z_vals[...,:-1]) # [N_rays, N_samples-1]\n",
    "            upper = torch.cat([mids, z_vals[...,-1:]], -1) # Upper bound\n",
    "            lower = torch.cat([z_vals[...,:1], mids], -1) # Lower bound\n",
    "            # stratified samples in those intervals\n",
    "            t_rand = torch.rand(z_vals.shape) # N_rays, N_samples]\n",
    "\n",
    "            # Pytest, overwrite u with numpy's fixed random numbers\n",
    "            # Not using\n",
    "            if pytest:\n",
    "                np.random.seed(0)\n",
    "                t_rand = np.random.rand(*list(z_vals.shape))\n",
    "                t_rand = torch.Tensor(t_rand)\n",
    "            \n",
    "            # Choose z location\n",
    "            z_vals = lower + (upper - lower) * t_rand\n",
    "        \n",
    "        # Make (x,y,z)\n",
    "        # o + td\n",
    "        pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples, 3]\n",
    "\n",
    "        # Not use fine network\n",
    "        if N_importance <= 0:\n",
    "            raw, position_delta = network_query_fn(pts, viewdirs, frame_time, network_fn)\n",
    "            # Below, i will introduce 'raw2outputs' function\n",
    "            rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest=pytest)\n",
    "\n",
    "        else:\n",
    "            # use two coarse networks and use fine network\n",
    "            # False boolean\n",
    "            if use_two_models_for_fine:\n",
    "                raw, position_delta_0 = network_query_fn(pts, viewdirs, frame_time, network_fn)\n",
    "                rgb_map_0, disp_map_0, acc_map_0, weights, _ = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest=pytest)\n",
    "\n",
    "            else:\n",
    "                # not training(?)\n",
    "                with torch.no_grad():\n",
    "                    raw, _ = network_query_fn(pts, viewdirs, frame_time, network_fn)\n",
    "                    _, _, _, weights, _ = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest=pytest)\n",
    "            \n",
    "            # Nf sampling\n",
    "            z_vals_mid = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n",
    "            # Using the coarse network weights\n",
    "            z_samples = sample_pdf(z_vals_mid, weights[...,1:-1], N_importance, det=(perturb==0.), pytest=pytest) # PDF, CDF\n",
    "            z_samples = z_samples.detach()\n",
    "            z_vals, _ = torch.sort(torch.cat([z_vals, z_samples], -1), -1)\n",
    "    \n",
    "    # Make again (x,y,z)\n",
    "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples + N_importance, 3]\n",
    "    # Fine network\n",
    "    run_fn = network_fn if network_fine is None else network_fine\n",
    "    raw, position_delta = network_query_fn(pts, viewdirs, frame_time, run_fn)\n",
    "    rgb_map, disp_map, acc_map, weights, _ = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest=pytest)\n",
    "    \n",
    "    # Make output\n",
    "    ret = {'rgb_map' : rgb_map, 'disp_map' : disp_map, 'acc_map' : acc_map, 'z_vals' : z_vals,\n",
    "           'position_delta' : position_delta}\n",
    "    if retraw:\n",
    "        ret['raw'] = raw\n",
    "    if N_importance > 0:\n",
    "        if rgb_map_0 is not None:\n",
    "            ret['rgb0'] = rgb_map_0\n",
    "        if disp_map_0 is not None:\n",
    "            ret['disp0'] = disp_map_0\n",
    "        if acc_map_0 is not None:\n",
    "            ret['acc0'] = acc_map_0\n",
    "        if position_delta_0 is not None:\n",
    "            ret['position_delta_0'] = position_delta_0\n",
    "        if z_samples is not None:\n",
    "            ret['z_std'] = torch.std(z_samples, dim=-1, unbiased=False)  # [N_rays]\n",
    "\n",
    "    for k in ret:\n",
    "        if (torch.isnan(ret[k]).any() or torch.isinf(ret[k]).any()) and DEBUG:\n",
    "            print(f\"! [Numerical Error] {k} contains nan or inf.\")\n",
    "            \n",
    "    # (kyujinpy add)\n",
    "    # Memory over problem solution\n",
    "    del rgb_map, disp_map, acc_map, weights, _\n",
    "\n",
    "    return ret\n",
    "\n",
    "###################################\n",
    "# Ninth, sample_pdf function\n",
    "# Nf sampling, and using in fine network\n",
    "def sample_pdf(bins, weights, N_samples, det=False, pytest=False):\n",
    "    # Get pdf\n",
    "    weights = weights + 1e-5 # prevent nans\n",
    "    pdf = weights / torch.sum(weights, -1, keepdim=True)\n",
    "    cdf = torch.cumsum(pdf, -1)\n",
    "    cdf = torch.cat([torch.zeros_like(cdf[...,:1]), cdf], -1)  # (batch, len(bins))\n",
    "\n",
    "    # Take uniform samples\n",
    "    if det:\n",
    "        u = torch.linspace(0., 1., steps=N_samples)\n",
    "        u = u.expand(list(cdf.shape[:-1]) + [N_samples])\n",
    "    else:\n",
    "        u = torch.rand(list(cdf.shape[:-1]) + [N_samples])\n",
    "\n",
    "    # Pytest, overwrite u with numpy's fixed random numbers\n",
    "    if pytest:\n",
    "        np.random.seed(0)\n",
    "        new_shape = list(cdf.shape[:-1]) + [N_samples]\n",
    "        if det:\n",
    "            u = np.linspace(0., 1., N_samples)\n",
    "            u = np.broadcast_to(u, new_shape)\n",
    "        else:\n",
    "            u = np.random.rand(*new_shape)\n",
    "        u = torch.Tensor(u)\n",
    "\n",
    "    # Invert CDF\n",
    "    u = u.contiguous()\n",
    "    inds = torch.searchsorted(cdf, u, side='right')\n",
    "    below = torch.max(torch.zeros_like(inds-1), inds-1)\n",
    "    above = torch.min((cdf.shape[-1]-1) * torch.ones_like(inds), inds)\n",
    "    inds_g = torch.stack([below, above], -1)  # (batch, N_samples, 2)\n",
    "\n",
    "    # cdf_g = tf.gather(cdf, inds_g, axis=-1, batch_dims=len(inds_g.shape)-2)\n",
    "    # bins_g = tf.gather(bins, inds_g, axis=-1, batch_dims=len(inds_g.shape)-2)\n",
    "    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n",
    "    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n",
    "    bins_g = torch.gather(bins.unsqueeze(1).expand(matched_shape), 2, inds_g)\n",
    "\n",
    "    denom = (cdf_g[...,1]-cdf_g[...,0])\n",
    "    denom = torch.where(denom<1e-5, torch.ones_like(denom), denom)\n",
    "    t = (u-cdf_g[...,0])/denom\n",
    "    samples = bins_g[...,0] + t * (bins_g[...,1]-bins_g[...,0])\n",
    "\n",
    "    return samples\n",
    "\n",
    "#######################################\n",
    "# Tenth, raw2ourputs function\n",
    "# Very Important\n",
    "def raw2outputs(raw, z_vals, rays_d, raw_noise_std=0, white_bkgd=False, pytest=False):\n",
    "    \"\"\"Transforms model's predictions to semantically meaningful values.\n",
    "    Args:\n",
    "        raw: [num_rays, num_samples along ray, 4]. Prediction from model.\n",
    "        z_vals: [num_rays, num_samples along ray]. Integration time.\n",
    "        rays_d: [num_rays, 3]. Direction of each ray.\n",
    "    Returns:\n",
    "        rgb_map: [num_rays, 3]. Estimated RGB color of a ray.\n",
    "        disp_map: [num_rays]. Disparity map. Inverse of depth map.\n",
    "        acc_map: [num_rays]. Sum of weights along each ray.\n",
    "        weights: [num_rays, num_samples]. Weights assigned to each sampled color.\n",
    "        depth_map: [num_rays]. Estimated distance to object.\n",
    "    \"\"\"\n",
    "    # raw: [num_rays, num_samples along ray, 4]\n",
    "    \n",
    "    # Make density.\n",
    "    # It is impossible negative density.\n",
    "    # So apply ReLU\n",
    "    raw2alpha = lambda raw, dists, act_fn=F.relu: 1.-torch.exp(-act_fn(raw)*dists)\n",
    "    \n",
    "    # Calculate distance each z location\n",
    "    dists = z_vals[...,1:] - z_vals[...,:-1]\n",
    "    # Last elements, append infinite(?)\n",
    "    dists = torch.cat([dists, torch.Tensor([1e10]).expand(dists[...,:1].shape)], -1)  # [N_rays, N_samples]\n",
    "    \n",
    "    dists = dists * torch.norm(rays_d[...,None,:], dim=-1)\n",
    "    \n",
    "    # It is impossible negative rgb.\n",
    "    # So apply sigmoid\n",
    "    # 0~1\n",
    "    rgb = torch.sigmoid(raw[...,:3])  # [N_rays, N_samples, 3]\n",
    "    noise = 0.\n",
    "    if raw_noise_std > 0.:\n",
    "        noise = torch.randn(raw[...,3].shape) * raw_noise_std\n",
    "\n",
    "        # Overwrite randomly sampled data if pytest\n",
    "        if pytest:\n",
    "            np.random.seed(0)\n",
    "            noise = np.random.rand(*list(raw[...,3].shape)) * raw_noise_std\n",
    "            noise = torch.Tensor(noise)\n",
    "    \n",
    "    alpha = raw2alpha(raw[...,3] + noise, dists)  # [N_rays, N_samples]\n",
    "    # weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n",
    "    \n",
    "    # The T's first element must be 1.\n",
    "    # So concat, 1\n",
    "    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1)), 1.-alpha + 1e-10], -1), -1)[:, :-1]\n",
    "    rgb_map = torch.sum(weights[...,None] * rgb, -2)  # [N_rays, 3]\n",
    "\n",
    "    depth_map = torch.sum(weights * z_vals, -1) # z locations * weights\n",
    "    disp_map = 1./torch.max(1e-10 * torch.ones_like(depth_map), depth_map / torch.sum(weights, -1)) #  If depth = 0, 1/0.\n",
    "    acc_map = torch.sum(weights, -1) # all sum weight.\n",
    "    \n",
    "    # Blender dataset\n",
    "    # RGBA dimension.\n",
    "    # So, make white background.\n",
    "    # If not, black.\n",
    "    if white_bkgd:\n",
    "        rgb_map = rgb_map + (1.-acc_map[...,None])\n",
    "        # rgb_map = rgb_map + torch.cat([acc_map[..., None] * 0, acc_map[..., None] * 0, (1. - acc_map[..., None])], -1)\n",
    "\n",
    "    return rgb_map, disp_map, acc_map, weights, depth_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f592f0d7",
   "metadata": {},
   "source": [
    "# 5. Training function (also can Testing)\n",
    "- train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D-NeRF training function\n",
    "def train(args):\n",
    "    \n",
    "    # Load data\n",
    "\n",
    "    if args.dataset_type == 'blender':\n",
    "        images, poses, times, render_poses, render_times, hwf, i_split = load_blender_data(args.datadir, args.half_res, args.testskip)\n",
    "        print('Loaded blender', images.shape, render_poses.shape, hwf, args.datadir)\n",
    "        i_train, i_val, i_test = i_split\n",
    "\n",
    "        near = 2.\n",
    "        far = 6.\n",
    "\n",
    "        if args.white_bkgd:\n",
    "            images = images[...,:3]*images[...,-1:] + (1.-images[...,-1:])\n",
    "        else:\n",
    "            images = images[...,:3]\n",
    "\n",
    "        # images = [rgb2hsv(img) for img in images]\n",
    "\n",
    "    else:\n",
    "        print('Unknown dataset type', args.dataset_type, 'exiting')\n",
    "        return\n",
    "\n",
    "    min_time, max_time = times[i_train[0]], times[i_train[-1]]\n",
    "    assert min_time == 0., \"time must start at 0\"\n",
    "    assert max_time == 1., \"max time must be 1\"\n",
    "\n",
    "    # Cast intrinsics to right types\n",
    "    H, W, focal = hwf\n",
    "    H, W = int(H), int(W)\n",
    "    hwf = [H, W, focal]\n",
    "\n",
    "    if args.render_test:\n",
    "        render_poses = np.array(poses[i_test])\n",
    "        render_times = np.array(times[i_test])\n",
    "\n",
    "    # Create log dir and copy the config file\n",
    "    basedir = args.basedir\n",
    "    expname = args.expname\n",
    "    os.makedirs(os.path.join(basedir, expname), exist_ok=True)\n",
    "    f = os.path.join(basedir, expname, 'args.txt')\n",
    "    with open(f, 'w') as file:\n",
    "        for arg in sorted(vars(args)):\n",
    "            attr = getattr(args, arg)\n",
    "            file.write('{} = {}\\n'.format(arg, attr))\n",
    "    if args.config is not None:\n",
    "        f = os.path.join(basedir, expname, 'config.txt')\n",
    "        with open(f, 'w') as file:\n",
    "            file.write(open(args.config, 'r').read())\n",
    "\n",
    "    # Create nerf model\n",
    "    render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer = create_nerf(args)\n",
    "    \n",
    "    # AssertionError: If capturable=False, state_steps should not be CUDA tensors.\n",
    "    # Below code implementation\n",
    "    optimizer.param_groups[0]['capturable'] = True\n",
    "    \n",
    "    global_step = start\n",
    "\n",
    "    bds_dict = {\n",
    "        'near' : near,\n",
    "        'far' : far,\n",
    "    }\n",
    "    render_kwargs_train.update(bds_dict)\n",
    "    render_kwargs_test.update(bds_dict)\n",
    "\n",
    "    # Move testing data to GPU\n",
    "    render_poses = torch.Tensor(render_poses).to(device)\n",
    "    render_times = torch.Tensor(render_times).to(device)\n",
    "\n",
    "    # Short circuit if only rendering out from trained model\n",
    "    # Rendering and End!!\n",
    "    if args.render_only:\n",
    "        print('RENDER ONLY')\n",
    "        with torch.no_grad():\n",
    "            if args.render_test:\n",
    "                # render_test switches to test poses\n",
    "                images = images[i_test]\n",
    "            else:\n",
    "                # Default is smoother render_poses path\n",
    "                images = None\n",
    "\n",
    "            testsavedir = os.path.join(basedir, expname, 'renderonly_{}_{:06d}'.format('test' if args.render_test else 'path', start))\n",
    "            os.makedirs(testsavedir, exist_ok=True)\n",
    "            print('test poses shape', render_poses.shape)\n",
    "            \n",
    "            # rendering only\n",
    "            rgbs, _ = render_path(render_poses, render_times, hwf, args.chunk, render_kwargs_test, gt_imgs=images,\n",
    "                                  savedir=testsavedir, render_factor=args.render_factor, save_also_gt=True)\n",
    "            print('Done rendering', testsavedir)\n",
    "            imageio.mimwrite(os.path.join(testsavedir, 'video.mp4'), to8b(rgbs), fps=30, quality=8)\n",
    "\n",
    "            return\n",
    "\n",
    "    # Prepare raybatch tensor if batching random rays\n",
    "    N_rand = args.N_rand\n",
    "    use_batching = not args.no_batching\n",
    "    \n",
    "    # Sampling coordinates in images. Not one image. \n",
    "    if use_batching:\n",
    "        # For random ray batching\n",
    "        print('get rays')\n",
    "        rays = np.stack([get_rays_np(H, W, focal, p) for p in poses[:,:3,:4]], 0) # [N, ro+rd, H, W, 3]\n",
    "        print('done, concats')\n",
    "        rays_rgb = np.concatenate([rays, images[:,None]], 1) # [N, ro+rd+rgb, H, W, 3]\n",
    "        rays_rgb = np.transpose(rays_rgb, [0,2,3,1,4]) # [N, H, W, ro+rd+rgb, 3]\n",
    "        rays_rgb = np.stack([rays_rgb[i] for i in i_train], 0) # train images only\n",
    "        rays_rgb = np.reshape(rays_rgb, [-1,3,3]) # [(N-1)*H*W, ro+rd+rgb, 3]\n",
    "        rays_rgb = rays_rgb.astype(np.float32)\n",
    "        print('shuffle rays')\n",
    "        np.random.shuffle(rays_rgb)\n",
    "\n",
    "        print('done')\n",
    "        i_batch = 0\n",
    "\n",
    "    # Move training data to GPU\n",
    "    images = torch.Tensor(images).to(device)\n",
    "    poses = torch.Tensor(poses).to(device)\n",
    "    times = torch.Tensor(times).to(device)\n",
    "    if use_batching:\n",
    "        rays_rgb = torch.Tensor(rays_rgb).to(device)\n",
    "\n",
    "    N_iters = args.N_iter + 1\n",
    "    print('Begin')\n",
    "\n",
    "    # Summary writers\n",
    "    writer = SummaryWriter(os.path.join(basedir, 'summaries', expname))\n",
    "    \n",
    "    start = start + 1\n",
    "    for i in trange(start, N_iters):\n",
    "        time0 = time.time() # record training time\n",
    "\n",
    "        # Sample random ray batch\n",
    "        if use_batching:\n",
    "            raise NotImplementedError(\"Time not implemented\")\n",
    "\n",
    "            # Random over all images\n",
    "            batch = rays_rgb[i_batch:i_batch+N_rand] # [B, 2+1, 3*?]\n",
    "            batch = torch.transpose(batch, 0, 1)\n",
    "            batch_rays, target_s = batch[:2], batch[2]\n",
    "\n",
    "            i_batch += N_rand\n",
    "            if i_batch >= rays_rgb.shape[0]:\n",
    "                print(\"Shuffle data after an epoch!\")\n",
    "                rand_idx = torch.randperm(rays_rgb.shape[0])\n",
    "                rays_rgb = rays_rgb[rand_idx]\n",
    "                i_batch = 0\n",
    "\n",
    "        else:\n",
    "            # Random from one image\n",
    "            if i >= args.precrop_iters_time:\n",
    "                img_i = np.random.choice(i_train)\n",
    "            else:\n",
    "                # Limitations choice image(?)\n",
    "                skip_factor = i / float(args.precrop_iters_time) * len(i_train)\n",
    "                max_sample = max(int(skip_factor), 3)\n",
    "                img_i = np.random.choice(i_train[:max_sample])\n",
    "\n",
    "            target = images[img_i]\n",
    "            pose = poses[img_i, :3, :4] # poses is c2w matrix (N, 4, 4)\n",
    "            frame_time = times[img_i]\n",
    "\n",
    "            if N_rand is not None:\n",
    "                rays_o, rays_d = get_rays(H, W, focal, torch.Tensor(pose))  # (H, W, 3), (H, W, 3)\n",
    "                \n",
    "                # Use center cropping, and some iterations sampling only center coordinates.\n",
    "                if i < args.precrop_iters:\n",
    "                    dH = int(H//2 * args.precrop_frac)\n",
    "                    dW = int(W//2 * args.precrop_frac)\n",
    "                    coords = torch.stack(\n",
    "                        torch.meshgrid(\n",
    "                            torch.linspace(H//2 - dH, H//2 + dH - 1, 2*dH), \n",
    "                            torch.linspace(W//2 - dW, W//2 + dW - 1, 2*dW)\n",
    "                        ), -1)\n",
    "                    if i == start:\n",
    "                        print(f\"[Config] Center cropping of size {2*dH} x {2*dW} is enabled until iter {args.precrop_iters}\")                \n",
    "                \n",
    "                # Random sampling\n",
    "                else:\n",
    "                    coords = torch.stack(torch.meshgrid(torch.linspace(0, H-1, H), torch.linspace(0, W-1, W)), -1)  # (H, W, 2)\n",
    "\n",
    "                coords = torch.reshape(coords, [-1,2])  # (H * W, 2)\n",
    "                select_inds = np.random.choice(coords.shape[0], size=[N_rand], replace=False)  # (N_rand,)\n",
    "                select_coords = coords[select_inds].long()  # (N_rand, 2)\n",
    "                rays_o = rays_o[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n",
    "                rays_d = rays_d[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n",
    "                batch_rays = torch.stack([rays_o, rays_d], 0)\n",
    "                target_s = target[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n",
    "\n",
    "        #####  Core optimization loop  #####\n",
    "        # training\n",
    "        rgb, disp, acc, extras = render(H, W, focal, chunk=args.chunk, rays=batch_rays, frame_time=frame_time,\n",
    "                                                verbose=i < 10, retraw=True,\n",
    "                                                **render_kwargs_train)\n",
    "\n",
    "        # Default: False\n",
    "        # Using the delta_x loss. -> Using in PSNR metrics\n",
    "        if args.add_tv_loss:\n",
    "            frame_time_prev = times[img_i - 1] if img_i > 0 else None\n",
    "            frame_time_next = times[img_i + 1] if img_i < times.shape[0] - 1 else None\n",
    "\n",
    "            if frame_time_prev is not None and frame_time_next is not None:\n",
    "                if np.random.rand() > .5:\n",
    "                    frame_time_prev = None\n",
    "                else:\n",
    "                    frame_time_next = None\n",
    "\n",
    "            if frame_time_prev is not None:\n",
    "                rand_time_prev = frame_time_prev + (frame_time - frame_time_prev) * torch.rand(1)[0]\n",
    "                _, _, _, extras_prev = render(H, W, focal, chunk=args.chunk, rays=batch_rays, frame_time=rand_time_prev,\n",
    "                                                verbose=i < 10, retraw=True, z_vals=extras['z_vals'].detach(),\n",
    "                                                **render_kwargs_train)\n",
    "\n",
    "            if frame_time_next is not None:\n",
    "                rand_time_next = frame_time + (frame_time_next - frame_time) * torch.rand(1)[0]\n",
    "                _, _, _, extras_next = render(H, W, focal, chunk=args.chunk, rays=batch_rays, frame_time=rand_time_next,\n",
    "                                                verbose=i < 10, retraw=True, z_vals=extras['z_vals'].detach(),\n",
    "                                                **render_kwargs_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        img_loss = img2mse(rgb, target_s) # MSE loss\n",
    "\n",
    "        tv_loss = 0\n",
    "        if args.add_tv_loss:\n",
    "            if frame_time_prev is not None:\n",
    "                tv_loss += ((extras['position_delta'] - extras_prev['position_delta']).pow(2)).sum()\n",
    "                if 'position_delta_0' in extras:\n",
    "                    tv_loss += ((extras['position_delta_0'] - extras_prev['position_delta_0']).pow(2)).sum()\n",
    "            if frame_time_next is not None:\n",
    "                tv_loss += ((extras['position_delta'] - extras_next['position_delta']).pow(2)).sum()\n",
    "                if 'position_delta_0' in extras:\n",
    "                    tv_loss += ((extras['position_delta_0'] - extras_next['position_delta_0']).pow(2)).sum()\n",
    "            tv_loss = tv_loss * args.tv_loss_weight\n",
    "\n",
    "        loss = img_loss + tv_loss\n",
    "        psnr = mse2psnr(img_loss) # Calculate PSNR\n",
    "\n",
    "        if 'rgb0' in extras:\n",
    "            img_loss0 = img2mse(extras['rgb0'], target_s)\n",
    "            loss = loss + img_loss0\n",
    "            psnr0 = mse2psnr(img_loss0)\n",
    "\n",
    "        if args.do_half_precision:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # NOTE: IMPORTANT!\n",
    "        ###   update learning rate   ###\n",
    "        decay_rate = 0.1\n",
    "        decay_steps = args.lrate_decay * 1000\n",
    "        new_lrate = args.lrate * (decay_rate ** (global_step / decay_steps))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = new_lrate\n",
    "        ################################\n",
    "\n",
    "        dt = time.time()-time0\n",
    "        # print(f\"Step: {global_step}, Loss: {loss}, Time: {dt}\")\n",
    "        #####           end            #####\n",
    "\n",
    "        # Rest is logging\n",
    "        if i%args.i_weights==0:\n",
    "            path = os.path.join(basedir, expname, '{:06d}.tar'.format(i))\n",
    "            save_dict = {\n",
    "                'global_step': global_step,\n",
    "                'network_fn_state_dict': render_kwargs_train['network_fn'].state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }\n",
    "            if render_kwargs_train['network_fine'] is not None:\n",
    "                save_dict['network_fine_state_dict'] = render_kwargs_train['network_fine'].state_dict()\n",
    "\n",
    "            if args.do_half_precision:\n",
    "                save_dict['amp'] = amp.state_dict()\n",
    "            torch.save(save_dict, path)\n",
    "            print('Saved checkpoints at', path)\n",
    "\n",
    "        if i % args.i_print == 0:\n",
    "            tqdm_txt = f\"[TRAIN] Iter: {i} Loss_fine: {img_loss.item()} PSNR: {psnr.item()}\"\n",
    "            if args.add_tv_loss:\n",
    "                tqdm_txt += f\" TV: {tv_loss.item()}\"\n",
    "            tqdm.write(tqdm_txt)\n",
    "\n",
    "            writer.add_scalar('loss', img_loss.item(), i)\n",
    "            writer.add_scalar('psnr', psnr.item(), i)\n",
    "            if 'rgb0' in extras:\n",
    "                writer.add_scalar('loss0', img_loss0.item(), i)\n",
    "                writer.add_scalar('psnr0', psnr0.item(), i)\n",
    "            if args.add_tv_loss:\n",
    "                writer.add_scalar('tv', tv_loss.item(), i)\n",
    "\n",
    "        del loss, img_loss, psnr, target_s\n",
    "        if 'rgb0' in extras:\n",
    "            del img_loss0, psnr0\n",
    "        if args.add_tv_loss:\n",
    "            del tv_loss\n",
    "        del rgb, disp, acc, extras\n",
    "\n",
    "        if i%args.i_img==0:\n",
    "            torch.cuda.empty_cache()\n",
    "            # Log a rendered validation view to Tensorboard\n",
    "            img_i=np.random.choice(i_val)\n",
    "            target = images[img_i]\n",
    "            pose = poses[img_i, :3,:4]\n",
    "            frame_time = times[img_i]\n",
    "            with torch.no_grad():\n",
    "                rgb, disp, acc, extras = render(H, W, focal, chunk=args.chunk, c2w=pose, frame_time=frame_time,\n",
    "                                                    **render_kwargs_test)\n",
    "\n",
    "            psnr = mse2psnr(img2mse(rgb, target))\n",
    "            writer.add_image('gt', to8b(target.cpu().numpy()), i, dataformats='HWC')\n",
    "            writer.add_image('rgb', to8b(rgb.cpu().numpy()), i, dataformats='HWC')\n",
    "            writer.add_image('disp', disp.cpu().numpy(), i, dataformats='HW')\n",
    "            writer.add_image('acc', acc.cpu().numpy(), i, dataformats='HW')\n",
    "\n",
    "            if 'rgb0' in extras:\n",
    "                writer.add_image('rgb_rough', to8b(extras['rgb0'].cpu().numpy()), i, dataformats='HWC')\n",
    "            if 'disp0' in extras:\n",
    "                writer.add_image('disp_rough', extras['disp0'].cpu().numpy(), i, dataformats='HW')\n",
    "            if 'z_std' in extras:\n",
    "                writer.add_image('acc_rough', extras['z_std'].cpu().numpy(), i, dataformats='HW')\n",
    "\n",
    "            print(\"finish summary\")\n",
    "            writer.flush()\n",
    "\n",
    "        if i%args.i_video==0:\n",
    "            # Turn on testing mode\n",
    "            print(\"Rendering video...\")\n",
    "            with torch.no_grad():\n",
    "                savedir = os.path.join(basedir, expname, 'frames_{}_spiral_{:06d}_time/'.format(expname, i))\n",
    "                rgbs, disps = render_path(render_poses, render_times, hwf, args.chunk, render_kwargs_test, savedir=savedir)\n",
    "            print('Done, saving', rgbs.shape, disps.shape)\n",
    "            moviebase = os.path.join(basedir, expname, '{}_spiral_{:06d}_'.format(expname, i))\n",
    "            imageio.mimwrite(moviebase + 'rgb.mp4', to8b(rgbs), fps=30, quality=8)\n",
    "            imageio.mimwrite(moviebase + 'disp.mp4', to8b(disps / np.max(disps)), fps=30, quality=8)\n",
    "\n",
    "            # if args.use_viewdirs:\n",
    "            #     render_kwargs_test['c2w_staticcam'] = render_poses[0][:3,:4]\n",
    "            #     with torch.no_grad():\n",
    "            #         rgbs_still, _ = render_path(render_poses, hwf, args.chunk, render_kwargs_test)\n",
    "            #     render_kwargs_test['c2w_staticcam'] = None\n",
    "            #     imageio.mimwrite(moviebase + 'rgb_still.mp4', to8b(rgbs_still), fps=30, quality=8)\n",
    "\n",
    "        if i%args.i_testset==0:\n",
    "            testsavedir = os.path.join(basedir, expname, 'testset_{:06d}'.format(i))\n",
    "            print('Testing poses shape...', poses[i_test].shape)\n",
    "            with torch.no_grad():\n",
    "                render_path(torch.Tensor(poses[i_test]).to(device), torch.Tensor(times[i_test]).to(device),\n",
    "                            hwf, args.chunk, render_kwargs_test, gt_imgs=images[i_test], savedir=testsavedir)\n",
    "            print('Saved test set')\n",
    "\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e7f90",
   "metadata": {},
   "source": [
    "# 6. Make config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95003fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parser\n",
    "parser = config_parser()\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# Changable parser\n",
    "args.config = './configs/hellwarrior.txt'\n",
    "args.expname = 'hellwarrior'\n",
    "args.basedir = './logs'\n",
    "args.datadir = './data/hellwarrior/'\n",
    "args.dataset_type = 'blender'\n",
    "\n",
    "# Default\n",
    "args.nerf_type = \"D_NeRF\"\n",
    "args.no_batching = True\n",
    "args.not_zero_canonical = False\n",
    "\n",
    "args.use_viewdirs = True\n",
    "args.white_bkgd = True\n",
    "args.lrate_decay = 500\n",
    "\n",
    "args.N_iter = 800000\n",
    "args.N_samples = 64 # Nc\n",
    "args.N_importance = 128 # Nf\n",
    "args.N_rand = 500\n",
    "args.testskip = 1\n",
    "\n",
    "args.precrop_iters = 500 # random image choose\n",
    "args.precrop_iters_time = 100000 # center cropping iterations\n",
    "args.precrop_frac = 0.5 # center cropping 할 때 범위\n",
    "\n",
    "args.half_res = True # low resoultion // I recommend False, but it takse log times..!\n",
    "args.do_half_precision = False\n",
    "\n",
    "args.no_reload = True # Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ea1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac8564",
   "metadata": {},
   "source": [
    "# 7. Implementation\n",
    "- hellwarrior dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a5c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First train\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a06d4",
   "metadata": {},
   "source": [
    "# 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b65043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make video\n",
    "# Sava in (basedir + expname + ...)\n",
    "args.no_reload = False\n",
    "args.render_only = True\n",
    "args.render_test = False\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b713515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimv",
   "language": "python",
   "name": "aimv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
